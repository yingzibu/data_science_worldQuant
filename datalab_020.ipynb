{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMjUNEvux80vod6f6gU88/+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yingzibu/data_science_worldQuant/blob/main/datalab_020.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Null\n",
        "\n",
        "Check null value counts: `df.isnull().sum()`\n",
        "\n",
        "Check null percentage: `df.isnull().sum()/len(df)`\n",
        "\n",
        "```\n",
        "df.describe()\n",
        "df.info()\n",
        "```\n",
        "\n",
        "# select categorical\n",
        "\n",
        "`df.select_dtypes('object')`\n",
        "\n",
        "`df.select_dtypes('object').nunique()`\n",
        "\n",
        "# Splitting strings\n",
        "\n",
        "It might be useful to split strings into their constituent parts, and create new columns to contain them. we will use `.str.split` method.\n",
        "\n",
        "```\n",
        "df[[\"col1\", \"col2\"]] = df[\"col1-col2\"].str.split(\",\", expand=True)\n",
        "```\n",
        "\n",
        "# Recasting data\n",
        "\n",
        "Depending on who formatted the dataset, the types of data assigned to each column might need to be changed. If, for example, a column containing only numbers had been mistaken for a column containing only strings, we'd need to change that through a process called recasting `astype` method\n",
        "\n",
        "```\n",
        "newdf = df.astype('str')\n",
        "```\n",
        "\n",
        "Only recasting individual columns\n",
        "\n",
        "```\n",
        "df['col'] = df['col'].astype(int)\n",
        "```\n",
        "\n",
        "# Dropping columns\n",
        "\n",
        "`drop`\n",
        "\n",
        "```\n",
        "df2 = df.drop(\"col_name\", axis='columns')\n",
        "\n",
        "df.dropna(inplace=True)\n",
        "```\n",
        "\n",
        "# Concatenating\n",
        "\n",
        "## Concatenating DataFrames\n",
        "\n",
        "```\n",
        "concat_df = pd.concat([df1, df2])\n",
        "```\n",
        "\n",
        "# Replacing col values\n",
        "```\n",
        "df['col'].replace(old_val, new_val)\n",
        "dict_ = {old: new}\n",
        "df['col'].replace(dict_)\n",
        "```"
      ],
      "metadata": {
        "id": "lnbWmLSovR5s"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# subsetting with masks\n",
        "\n",
        "To create subsets from a larger dataset is through **masking**. Masks are ways to filter out the data you are not interested in so that you can focus on the data you are.\n",
        "\n",
        "```\n",
        "mask = df[col] > 200\n",
        "mask\n",
        "```\n",
        "Notice that `mask` is a Series of Boolean values. Where properties are smaller than 200, our statement evaluates as `False`; where they are bigger than 200, it evaluates to `True`.\n",
        "\n",
        "```\n",
        "df[mask]\n",
        "```"
      ],
      "metadata": {
        "id": "JeEp-vFfFEDN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Histograms\n",
        "\n",
        "A **histogram** is a graph that shows the frequency distribution of numerical data. In addition to helping us understand frequency, histograms are also useful for detecting outliers.\n",
        "\n",
        "```\n",
        "df = pd.read_csv(file_path, usecols = ['col_name'])\n",
        "plt.hist(df, bins=10, rwidth=0.9, color='b')\n",
        "plt.title(title)\n",
        "plt.xlabel(xlabel)\n",
        "plt.ylabel(ylabel)\n",
        "plt.grid(axis='y', alpha = 0.75)\n",
        "```\n",
        "\n",
        "You might have noticed that there are ten bars. In a histogram, we call these bars **bins**. A bin is simply a way to group data to make it easier to see trends. You can use as many or as few as you like; just recognize that the fewer bins you use, the less detailed the output will become.\n",
        "\n"
      ],
      "metadata": {
        "id": "VONx-MVfGm9m"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Scatter plots\n",
        "\n",
        "A **scatter plot** is a graph that uses dots to represent values for two different numeric variables. The position of each dot on the horizontal and vertical axis indicates values for an individual data point. Scatter plots are used to observe relationships between variables, and are especially useful if you're looking for correlations.\n",
        "\n",
        "```\n",
        "plt.scatter(df['col1'], df['col2'], color = 'r')\n",
        "plt.xlabel(xlabel)\n",
        "plt.ylabel(ylabel)\n",
        "plt.title(title)\n",
        "```"
      ],
      "metadata": {
        "id": "ewhUTcomPRwy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Quantiles\n",
        "\n",
        "```\n",
        "low, high = df['col_name'].quantile([0.1, 0.9])\n",
        "mask = df['col_name'].between(low, high)\n",
        "df[mask]\n",
        "```\n",
        "\n",
        "# Contain strings\n",
        "\n",
        "```\n",
        "mask = df['col_name'].str.contains('strings')\n",
        "df[mask]\n",
        "```"
      ],
      "metadata": {
        "id": "GsdM8pc5NpVg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Line plots\n",
        "\n",
        "```\n",
        "df = pd.DataFrame({'x_coords': range(0, 9000, 1000)})\n",
        "df['y_coords'] = y0 + k * df['x_coords']\n",
        "df.plot(x='x_coords', y='y_coords', xlabel=xlabel, ylabel=ylabel, label=label)\n",
        "```"
      ],
      "metadata": {
        "id": "gkh8or2dULjs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Linear Regression\n",
        "\n",
        "```\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "model = LinearRegression()\n",
        "\n",
        "model.fit(X_train, y_train)\n",
        "y_pred = model.predict(X_train)\n",
        "intercept = model.intercept_\n",
        "coefficient = model.coef_\n",
        "\n",
        "y_line = intercept + coefficient * X_train.values\n",
        "\n",
        "plt.plot(X_train.values, y_line, label='Linear Regression Model')\n",
        "plt.scatter(X_train, y_train)\n",
        "plt.xlabel(xlabel)\n",
        "plt.ylabel(ylabel)\n",
        "plt.title(title)\n",
        "plt.legend()\n",
        "\n",
        "```"
      ],
      "metadata": {
        "id": "0pdhltVnfB0Y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model types\n",
        "\n",
        "**Linear Regression** is a way to predict the value of some a target variable by fitting a line that best describes the relationship between **X** and **y** for the values we already have. If you remember `y = mx + b`, `y` is the intercept, and the `b` is the beta coefficient. The beta coefficient tells us what change we can expect to see in `X` for every one-unit increase in `y`.\n",
        "\n",
        "## Statistical concepts\n",
        "\n",
        "### Cost functions\n",
        "\n",
        "When we train a model, we are solving an optimization problem. We provide training data to an algorithm and tell it to find the model or model parameters that best fit the data. But how can the algorithm judge what the \"best\" fit is? What criteria should it use?\n",
        "\n",
        "A **cost function** (sometimes also called a loss or error function) is a mathematical formula that provides the score by whic the algorithm will determine the best fit. Generally, the goal is to minimize the cost function and get the lowest score. For linear models, these functions measure distance, and the model tries to get the closest fit to the data. For tree-based model, they measure impurity, and the model tries to get the most terminal nodes.\n",
        "\n",
        "### Residuals\n",
        "\n",
        "When we perform any type of regression analysis, we end up with a line of best fit. Because our data comes from the real world, it tends to be a little bit messy, so the data points usually dont fall exactly on this line. Most of the time, they are scattered around it, and a residual is the vertical distance between each individual data point and the regression line. Each data point has only one residual which can be positive if it's above the regression line, negative if it's below the regression line, r zero if the line passes directly through the point. Think of it like this: the model describes theoretical line. That line doesn't really exist outside the model. The residuals, however, are true values; they represent the actual data that came from real observations.\n",
        "\n",
        "### Performance metrics\n",
        "\n",
        "In statistics, an error is the difference between a measurement and reality. There may not be any difference at all, but there's usually something not quite right, and we need to account for that in our model. To do that, we need to figure out the **mean absolute error (MAE)**. Absolute error is the error in a single measurement, and mean absolute error is the average error over the course of several measurements.\n",
        "\n",
        "## Data concepts\n",
        "\n",
        "### Leakage\n",
        "\n",
        "**Leakage** is the use of data in training your model that would not be typically be available when making predictions. For example, suppose we want to predict property prices in USD but include property prices in Mexican Pesos in our model. If we assume a fixed exchange rate or a nearly constant exchange rate, then our model will have a low error on the training data, but this will not be reflective of its performance on real world data.\n",
        "\n",
        "### Imputation\n",
        "\n",
        "Datasets are often incomplete or missing entries. If the dataset is large and the missing entries are few, then the missing entries aren't all that important. But sometimes, it might be useful to include data with missing entries by finding a way to **impute** the missing entries in a row or column of a DataFrame. For example, you might use extrapolation when the data points have a pattern, or you might approximate the missing value by mean values.\n",
        "`SimpleImputer`\n",
        "\n",
        "```\n",
        "from sklearn.impute import SimpleImputer\n",
        "columns = ['col1', 'col2']\n",
        "df = pd.read_csv(filepath, usecols=columns)\n",
        "imputer = SimpleImputer()\n",
        "imputer.fit(df)\n",
        "imputed = imputer.transform(df)\n",
        "# convert into DataFrame\n",
        "df = pd.DataFrame(imputed, columns=columns)\n",
        "\n",
        "```\n",
        "### Generalization\n",
        "\n",
        "Notice that we tested the model with a dataset that is different from the one we used to train the model. Machine learning models are useful if they allow you to make predictions about data other than what you used to train your model. We call this concept **generalization**. By testing your model with different data than you used to train it, you're checking to see if your model can generalize. Most machine learning models do not generalize to all possible types of input data, so they should be used with care. On the other hand, machine learning models that don't generalize to make predictions for at least a restricted set of data aren't very useful.\n",
        "\n",
        "## Model concepts\n",
        "\n",
        "### Hyperparameters\n",
        "\n",
        "When we instantiate an estimator, we can pass keyword arguments that will dictate its structure. These arguments are called **hyperparameters**. For example, when we defined our decision tree estimator, we chose how many layers the tree would have using the `max_depth` keyword. This is in contrast to **parameters**, which are the numbers that our model used to make predictions based on features. Parameters are optimized during the training process based on data and input features. They keep changing during training to fit the data and only the best performed ones were selected. Hyperparameters values are set before training begins and will not be changed during the training process Pretty much all models have hyperparameters. Even a simple linear regressor has a hyperparameter: `fit_intercept`. Here are some common examples for hyperparameters:\n",
        "\n",
        "* The imputation strategy used for missing data\n",
        "* The number of trees in a random forest model\n",
        "* The number of jobs to run in parallel when fitting and predicting"
      ],
      "metadata": {
        "id": "0Wv-2CxYh4ys"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# plot scatter plot\n",
        "\n",
        "## Map\n",
        "```\n",
        "df['lon'] = df.lon.astype(float)\n",
        "df['lat'] = df.lat.astype(float)\n",
        "fig = px.scatter_mapbox(\n",
        "    df,\n",
        "    lat=df['lat'],\n",
        "    lon=df['lon'],\n",
        "    width=600,\n",
        "    height=600,\n",
        "    hover_data=['price_aprox_usd'],\n",
        ")\n",
        "fig.update_layout(mapbox_style='open-street-map')\n",
        "fig.show()\n",
        "\n",
        "\n",
        "```\n",
        "\n",
        "## 3D scatter plot\n",
        "\n",
        "```\n",
        "fig = px.scatter_3d(\n",
        "    df,\n",
        "    lat=df['lat'],\n",
        "    lon=df['lon'],\n",
        "    z=df['price_aprox_usd'],\n",
        "    labels = {\"lon\":\"longitude\", \"lat\":\"latitude\", \"price_aprox_usd\":\"price\"},\n",
        "    width=600,\n",
        "    height=500,\n",
        "    \n",
        ")\n",
        "fig.update_traces(\n",
        "    marker={\"size\":4, \"line\":{\"width\":2, \"color\":\"DarkSlateGrey\"}},\n",
        "    selector={\"mode\":\"markers\"},\n",
        ")\n",
        "fig.show()\n",
        "\n",
        "```\n"
      ],
      "metadata": {
        "id": "ZnHy-kWFzkxX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Creating a pipeline in scikit-learn"
      ],
      "metadata": {
        "id": "DwpKL8pY37fr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install category_encoders --quiet"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vC18xxNZRdOe",
        "outputId": "db00a595-8a55-45b6-c7c4-837f7039bb96"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/85.9 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.9/85.9 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn import linear_model\n",
        "from sklearn.pipeline import Pipeline\n",
        "from category_encoders import OneHotEncoder\n",
        "lin_reg = linear_model.LinearRegression()\n",
        "pipe = Pipeline([('ohe', OneHotEncoder(use_cat_names=True)),\n",
        "                    (\"regressor\", lin_reg)], )\n",
        "# feature_names = pipe.named_steps['ohe'].get_feature_names()\n",
        "pipe.named_steps"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dsJBmtIlN17u",
        "outputId": "c3f6115b-125e-4c6a-bac8-bb2c71662861"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'ohe': OneHotEncoder(use_cat_names=True), 'regressor': LinearRegression()}"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# glob\n",
        "\n",
        "```\n",
        "import glob\n",
        "\n",
        "glob.glob(\"./data/ada-[0-9].csv\")\n",
        "glob.glob(\"./data/ada*\")\n",
        "```\n",
        "\n",
        "It's not difficult to find the ones we want. The `.glob` function allows for pattern matching. Here are a few of the more common ones:\n",
        "\n",
        "* `*` match any number of characters\n",
        "* `?` match a single character of any kind\n",
        "* `[a-z]` match any lower case alphabetical character in the current locale\n",
        "* `[A-Z]` match any upper case alphabetical character in the current locale\n",
        "* `[!a-z]` do not match any lower case alphabetical character in the current locale\n",
        "\n",
        "So far, you have only searched for files in one specific directory `data`, it's also possible to search for files in subdirectories. To get a listing of all notebook files starting from the directory above this one and all others below it, can use:\n",
        "\n",
        "```\n",
        "glob.glob(\"../**/*.ipynb\", recursive=True)\n",
        "```"
      ],
      "metadata": {
        "id": "jHX7e8Ze7tEP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# One-hot encoding\n",
        "\n",
        "A property's district is **categorical data**, or data which can be divided into groups. For many machine learning algorithms, it's common to create a column in a DataFrame to indicate if the feature is present or absent, instead of using the category's name. For each observation, you put a 1 or a 0 to indicate if the property is located in each neighborhood or not.\n",
        "\n",
        "```\n",
        "from category_encoders import OneHotEncoder\n",
        "ohe = OneHotEncoder(use_cat_names=True)\n",
        "ohe.fit(df)\n",
        "df_ohe = ohe.transform(df)\n",
        "```\n",
        "\n",
        "# Ordinal encoding\n",
        "\n",
        "For many ML algorithms, it's common to use one-hot encoding. This works well if there are a few categories, but as the number of feature grows, the number of additional columns also grows.\n",
        "\n",
        "Having a large number of columns (and consequently a large number of features in your model) can lead to a number of issues often referred to as the **curse of dimensionality**. Two primary issues that can arise are computational complexity (operations performed on large datasets may take longer) and overfitting (the model may not generalize to new data). In these scenarios, ordinal encoding is a popular choice for encoding the categorical variable. Instead of creating new columns, ordinal encoding simply replaces the categories in a categorical variable with integers.\n",
        "\n",
        "One potential risk of ordinal encoding is that some ML algorithms assume the integer values imply an ordering the variables. This is important in logistic regression, where a relationship is defined between increases or decreases in the features and the target. Techniques like decision trees are okay to use ordinal encoding, because they generate splits. Rather than assuming any ordering between the numeric values, the splits will occur between the numeric values and effectively separate them.\n",
        "\n",
        "```\n",
        "from category_encoders import OrdinalEncoder\n",
        "oe = OrdinalEncoder()\n",
        "oe.fit(df)\n",
        "X_train_oe = oe.transform(df)\n",
        "```"
      ],
      "metadata": {
        "id": "D8KGRJmNC6-y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Ridge regression\n",
        "\n",
        "Sometimes the values for coefficients and the intercept - both positive and negative - are very large. When you see this in a linear model - especially a high-dimensional model - what's happening is that the model is overfitting to the training data and then can't generalize to the test data. Some people call this the curse of dimensionality.\n",
        "\n",
        "The way to solve this problem is to use regularization, a group of techniques that prevent overfitting. In this case, we will change the predictor from `LinearRegression` to `Ridge`, which is a linear regressor with an added tool for keeping model coefficients from getting too big."
      ],
      "metadata": {
        "id": "4nXr4gMDSsKU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Multicollinearity\n",
        "\n",
        "Two features have high correlation with each other.\n"
      ],
      "metadata": {
        "id": "HcLFBETWebNH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from category_encoders import OneHotEncoder\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.linear_model import Ridge\n",
        "from sklearn.pipeline import make_pipeline\n",
        "\n",
        "model = make_pipeline(\n",
        "    OneHotEncoder(use_cat_names=True), # encode for categorical data\n",
        "    SimpleImputer(), # impute missing values\n",
        "    Ridge(), # regularization\n",
        ")\n",
        "# model.fit(X_train, y_train)\n",
        "# y_pred = model.predict(X_train)"
      ],
      "metadata": {
        "id": "tmFJSaWe4M5c"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "```\n",
        "def wrangle(filepath):\n",
        "    # Read CSV file\n",
        "    df = pd.read_csv(filepath)\n",
        "\n",
        "    # Subset data: Apartments in \"Capital Federal\", less than 400,000\n",
        "    mask_ba = df[\"place_with_parent_names\"].str.contains(\"Capital Federal\")\n",
        "    mask_apt = df[\"property_type\"] == \"apartment\"\n",
        "    mask_price = df[\"price_aprox_usd\"] < 400_000\n",
        "    df = df[mask_ba & mask_apt & mask_price]\n",
        "\n",
        "    # Subset data: Remove outliers for \"surface_covered_in_m2\"\n",
        "    low, high = df[\"surface_covered_in_m2\"].quantile([0.1, 0.9])\n",
        "    mask_area = df[\"surface_covered_in_m2\"].between(low, high)\n",
        "    df = df[mask_area]\n",
        "\n",
        "    # Split \"lat-lon\" column\n",
        "    df[[\"lat\", \"lon\"]] = df[\"lat-lon\"].str.split(\",\", expand=True).astype(float)\n",
        "    df.drop(columns=\"lat-lon\", inplace=True)\n",
        "\n",
        "    # Get place name\n",
        "    df[\"neighborhood\"] = df[\"place_with_parent_names\"].str.split(\"|\", expand=True)[3]\n",
        "    df.drop(columns=\"place_with_parent_names\", inplace=True)\n",
        "\n",
        "   \n",
        "    return df\n",
        "```"
      ],
      "metadata": {
        "id": "RLG4ABwAm8qF"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-w3ukw95hqPb"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}